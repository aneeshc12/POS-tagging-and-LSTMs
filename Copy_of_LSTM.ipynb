{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aneeshc12/POS-tagging-and-LSTMs/blob/master/Copy_of_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# INLP A2 2020111018"
      ],
      "metadata": {
        "id": "WSKSo1GpTz3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "0rKq6mkEUAgU",
        "outputId": "dd86803e-b324-45de-a887-d3757e05577d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "from copy import copy"
      ],
      "metadata": {
        "id": "idYNya3YsY7m"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parse data\n",
        "\n",
        "# parse conllu, return sentences and tags\n",
        "def parseConllu(path):\n",
        "  X = []\n",
        "  y = []\n",
        "  pairs = []\n",
        "\n",
        "  with open(path) as f:\n",
        "    conllu = f.read().split('\\n\\n')\n",
        "    for block in conllu:\n",
        "      lines = block.split('\\n')\n",
        "      \n",
        "      sentence = []\n",
        "      tags = []\n",
        "      for line in lines[2:]:\n",
        "        items = line.split('\\t')\n",
        "        sentence.append(items[1])\n",
        "        tags.append(items[3])\n",
        "\n",
        "      pairs.append([sentence, tags])\n",
        "      \n",
        "  return np.array(pairs, dtype=object)\n",
        "\n",
        "# generate mappings between indices and words in sequences\n",
        "def assembleVocabulary(sequences, predefinedTags={\"_unk\": 0, \"_pad\": 1, \"_bos\": 2, \"_eos\": 3}):\n",
        "  vocab2idx = copy(predefinedTags)\n",
        "  count = len(predefinedTags)\n",
        "\n",
        "  # generate forward map\n",
        "  for sequence in sequences:\n",
        "    for word in sequence:\n",
        "      if word not in vocab2idx:\n",
        "        vocab2idx[word] = count\n",
        "        count += 1\n",
        "\n",
        "  # backwards map\n",
        "  idx2vocab = {vocab2idx[k]: k for k in vocab2idx}\n",
        "\n",
        "  return vocab2idx, idx2vocab\n",
        "\n",
        "# encode a sequence of words as a float tensor, takes a sentence and a dict as inputs, return a float tensor\n",
        "def encodeSequence(seq, toIdx):\n",
        "  encoded = []\n",
        "  for word in seq:\n",
        "    if word not in toIdx:\n",
        "      encoded.append(toIdx[\"_unk\"])\n",
        "    else:\n",
        "      encoded.append(toIdx[word])\n",
        "  encoded = torch.FloatTensor(encoded)\n",
        "  return encoded\n",
        "\n",
        "# pad all sequences with a \"_pad\" character\n",
        "def padSequence(sequences):\n",
        "  maxLength = 0\n",
        "  paddedSeqs = []\n",
        "\n",
        "  for seq in sequences:\n",
        "    if len(seq) > maxLength:\n",
        "      maxLength = len(seq)\n",
        "\n",
        "  for seq in sequences:\n",
        "    paddingNeeded = maxLength - len(seq)\n",
        "    paddedSeqs.append(seq + [\"_pad\"] * paddingNeeded)\n",
        "\n",
        "  paddedSeqs = np.array(paddedSeqs, dtype=object)\n",
        "  return paddedSeqs\n",
        "\n",
        "# begin and end sentences with \"_bos\" and \"_eos\" characters\n",
        "def delimitSequence(sequences):\n",
        "  delimited = []\n",
        "  for seq in sequences:\n",
        "    delimited.append([\"_bos\"] + seq + [\"_eos\"])\n",
        "\n",
        "  delimited = np.array(delimited, dtype=object)\n",
        "  return delimited"
      ],
      "metadata": {
        "id": "ZapBYceFugFW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# parse data, generate training, dev and test splits\n",
        "\n",
        "# load shuffle and preprocess data\n",
        "pairs = parseConllu(\"/content/drive/MyDrive/ud-english-treebanks/UD_English-Atis/en_atis-ud-train.conllu\")\n",
        "\n",
        "np.random.shuffle(pairs)\n",
        "\n",
        "X = (pairs[:, 0])\n",
        "y = (pairs[:, 1])\n",
        "\n",
        "X = delimitSequence(X)\n",
        "y = delimitSequence(y)\n",
        "\n",
        "X = padSequence(X)\n",
        "y = padSequence(y)\n",
        "\n",
        "# make vocabs\n",
        "word2idx, idx2word = assembleVocabulary(X)\n",
        "tag2idx, idx2tag = assembleVocabulary(y)\n",
        "\n",
        "# split data\n",
        "trainAmt = 0.7\n",
        "devAmt = 0.1\n",
        "\n",
        "trainIdx = int(trainAmt * pairs.shape[0])\n",
        "devIdx = int(devAmt * pairs.shape[0])\n",
        "\n",
        "trainX = X[:trainIdx]\n",
        "trainY = y[:trainIdx]\n",
        "\n",
        "devX = X[trainIdx:(trainIdx + devIdx)]\n",
        "devY = y[trainIdx:(trainIdx + devIdx)]\n",
        "\n",
        "testX = X[(trainIdx + devIdx):]\n",
        "testY = y[(trainIdx + devIdx):]"
      ],
      "metadata": {
        "id": "8I0rCLM49UyD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create dataloaders\n",
        "\n",
        "# encode sequences internally \n",
        "class POSTagDataset(Dataset):\n",
        "  def __init__(self, sentences, tags, word2idx, tag2idx):\n",
        "    # encode and store sentences\n",
        "    encSentences = torch.Tensor(encodeSequence(sentences[0], word2idx))\n",
        "    for sentence in sentences[1:]:\n",
        "      encSentences = torch.vstack([encSentences, encodeSequence(sentence, word2idx)])\n",
        "    \n",
        "    self.encSentences = encSentences\n",
        "\n",
        "    # encode and store POS tags\n",
        "    encPOS = torch.Tensor(encodeSequence(tags[0], tag2idx))\n",
        "    for tag in tags[1:]:\n",
        "      encPOS = torch.vstack([encPOS, encodeSequence(tag, tag2idx)])\n",
        "    \n",
        "    self.encPOS = encPOS\n",
        "    assert(self.encPOS.shape[0] == self.encSentences.shape[0])\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.encSentences.shape[0]\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.encSentences[idx], self.encPOS[idx]"
      ],
      "metadata": {
        "id": "Ik0ZzdF0HGEz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainDataset = POSTagDataset(trainX, trainY, word2idx, tag2idx)\n",
        "devDataset = POSTagDataset(devX, devY, word2idx, tag2idx)\n",
        "testDataset = POSTagDataset(testX, testY, word2idx, tag2idx)\n",
        "\n",
        "print(trainX.shape)\n",
        "print(trainY.shape)"
      ],
      "metadata": {
        "id": "LNMW8TYLJUiN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38a335f3-b177-4e6a-8ce3-538b287886c8"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2992, 48)\n",
            "(2992, 48)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define main LSTM and experiment classes\n",
        "\n",
        "# main lstm, take in encoded sente\n",
        "class LSTM(nn.Module):\n",
        "  def __init__(self, embeddingDim, hiddenDim, wordVocab, tagVocab):\n",
        "    super(LSTM, self).__init__()\n",
        "\n",
        "\n",
        "    self.hiddenDim = hiddenDim\n",
        "    self.padIdx = 1\n",
        "    self.wordEmbeddings = nn.Embedding(len(wordVocab), embeddingDim, \n",
        "                                       padding_idx=self.padIdx)         # encode each word as an embedding of size embeddingDim\n",
        "\n",
        "    self.lstm = nn.LSTM(embeddingDim, hiddenDim, batch_first=True)      # main lstm \n",
        "    self.fc = nn.Linear(hiddenDim, len(tagVocab))                       # output a tag based on the hidden dim (using the internal state)\n",
        "\n",
        "    self.logSoftMax = F.log_softmax\n",
        "\n",
        "  def forward(self, sentences):\n",
        "    embeddings = self.wordEmbeddings(sentences)\n",
        "    out, _ = self.lstm(embeddings)\n",
        "    out = self.fc(out)\n",
        "\n",
        "    tagScores = self.logSoftMax(out, dim=1)\n",
        "    return tagScores\n",
        "    \n",
        "# experiment class to manage training and testing\n",
        "class Experiment():\n",
        "  def __init__(self, embeddingDim, hiddenDim, wordVocab, tagVocab, batchSize=16, lossFunction=nn.CrossEntropyLoss, optimiser=torch.optim.SGD, lr=0.01):\n",
        "    self.model = LSTM(embeddingDim, hiddenDim, wordVocab, tagVocab)\n",
        "    self.lossFunction = lossFunction()\n",
        "    self.optimiser = optimiser(self.model.parameters(), lr=lr)\n",
        "\n",
        "    self.batchSize = batchSize\n",
        "\n",
        "  def train(self, trainX, trainY, devX, devY, numEpochs=50):\n",
        "    # init dataloaders\n",
        "    trainDataloader = DataLoader(trainDataset, batch_size=self.batchSize, shuffle=True)\n",
        "    devDataloader = DataLoader(devDataset, batch_size=self.batchSize, shuffle=True)\n",
        "\n",
        "    # iterate over all train batches, train the LSTM with sentences and labels\n",
        "    # keep iterating until performance on validation drops (early stoppage)\n",
        "    lastDevLoss = np.inf\n",
        "    for epoch in range(numEpochs):\n",
        "\n",
        "      # train over training data\n",
        "      trainingLoss = 0.0\n",
        "      for i, (sentences, labels) in enumerate(iter(trainDataloader)):\n",
        "        self.model.zero_grad()\n",
        "        tagScores = self.model(sentences.long())\n",
        "        \n",
        "        loss = self.lossFunction(tagScores.permute(0,2,1), labels.long())        # permute tagscores to calculate the loss over a sentence\n",
        "        loss.backward()\n",
        "        self.optimiser.step()\n",
        "\n",
        "        trainingLoss += loss\n",
        "      trainingLoss /= len(trainDataloader)\n",
        "\n",
        "      # evaluate on dev data\n",
        "      devLoss = 0.0\n",
        "      with torch.no_grad():\n",
        "        for i, (sentences, labels) in enumerate(iter(devDataloader)):\n",
        "          self.model.zero_grad()\n",
        "          tagScores = self.model(sentences.long())\n",
        "          \n",
        "          loss = self.lossFunction(tagScores.permute(0,2,1), labels.long())        # permute tagscores to calculate the loss over a sentence\n",
        "\n",
        "          devLoss += loss\n",
        "        devLoss /= len(devDataloader)\n",
        "\n",
        "      if epoch % 2 == 0:\n",
        "        print(\"Epoch %d | avg. training error: %f | avg. dev error: %f\" % (epoch, trainingLoss, devLoss))\n",
        "      \n",
        "      if devLoss > lastDevLoss:\n",
        "        print(\"Increase in dev loss, stopping training\")\n",
        "        break\n",
        "      else:\n",
        "        lastDevLoss = devLoss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# for i, (ts, tl) in enumerate(iter(trainDataloader)):\n",
        "#   print(\"iter \", i)\n",
        "#   print(ts.shape, ts[0])\n",
        "#   print(tl.shape, tl[0])\n",
        "\n",
        "trainDataloader = DataLoader(trainDataset, batch_size=16, shuffle=True)\n",
        "trainSentences, trainLabels = next(iter(trainDataloader))\n",
        "\n",
        "# trainSentences.view(trainSentences.shape[0], trainSentences.shape[1], 1),\n",
        "\n",
        "myLSTM = LSTM(30, 50, word2idx, tag2idx)\n",
        "# embs = myLSTM.wordEmbeddings(trainSentences.long())\n",
        "# res, _ = myLSTM.lstm(embs)\n",
        "# tagg = myLSTM.fc(res)\n",
        "# scores = myLSTM.logSoftMax(tagg)\n",
        "\n",
        "# scores = myLSTM(trainSentences.long())\n",
        "# loss = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "\n",
        "# print(\"score shep: \", scores.permute(0,2,1).shape)\n",
        "# print(\"llb shep: \", trainLabels.shape)\n",
        "# op = loss(scores.permute(0,2,1), trainLabels.long())\n",
        "\n",
        "e1 = Experiment(30, 50, word2idx, tag2idx)\n",
        "e1.train(trainX, trainY, devX, devY, numEpochs=100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 817
        },
        "id": "yHpURtuhOe3K",
        "outputId": "27d66544-cae5-412d-c9b7-799e73fe8f07"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 | avg. training error: 2.834647 | avg. dev error: 2.828374\n",
            "Epoch 2 | avg. training error: 2.808918 | avg. dev error: 2.802219\n",
            "Epoch 4 | avg. training error: 2.775244 | avg. dev error: 2.765501\n",
            "Epoch 6 | avg. training error: 2.720396 | avg. dev error: 2.703934\n",
            "Epoch 8 | avg. training error: 2.634596 | avg. dev error: 2.614754\n",
            "Epoch 10 | avg. training error: 2.541312 | avg. dev error: 2.523913\n",
            "Epoch 12 | avg. training error: 2.438318 | avg. dev error: 2.415653\n",
            "Epoch 14 | avg. training error: 2.288844 | avg. dev error: 2.246604\n",
            "Epoch 16 | avg. training error: 1.982841 | avg. dev error: 1.850685\n",
            "Epoch 18 | avg. training error: 1.192079 | avg. dev error: 1.030116\n",
            "Epoch 20 | avg. training error: 0.841808 | avg. dev error: 0.802546\n",
            "Epoch 22 | avg. training error: 0.758259 | avg. dev error: 0.737147\n",
            "Epoch 24 | avg. training error: 0.719347 | avg. dev error: 0.701685\n",
            "Epoch 26 | avg. training error: 0.693474 | avg. dev error: 0.677733\n",
            "Epoch 28 | avg. training error: 0.673405 | avg. dev error: 0.659013\n",
            "Epoch 30 | avg. training error: 0.656255 | avg. dev error: 0.642929\n",
            "Epoch 32 | avg. training error: 0.641044 | avg. dev error: 0.629678\n",
            "Epoch 34 | avg. training error: 0.627044 | avg. dev error: 0.614580\n",
            "Epoch 36 | avg. training error: 0.613838 | avg. dev error: 0.601770\n",
            "Epoch 38 | avg. training error: 0.601122 | avg. dev error: 0.589928\n",
            "Epoch 40 | avg. training error: 0.588613 | avg. dev error: 0.577611\n",
            "Epoch 42 | avg. training error: 0.576050 | avg. dev error: 0.565020\n",
            "Epoch 44 | avg. training error: 0.563181 | avg. dev error: 0.552252\n",
            "Epoch 46 | avg. training error: 0.549826 | avg. dev error: 0.539689\n",
            "Epoch 48 | avg. training error: 0.535872 | avg. dev error: 0.526268\n",
            "Epoch 50 | avg. training error: 0.521465 | avg. dev error: 0.513042\n",
            "Epoch 52 | avg. training error: 0.507028 | avg. dev error: 0.498217\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-4f93f23d9fa5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0me1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag2idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m \u001b[0me1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumEpochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-45-4f93f23d9fa5>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, trainX, trainY, devX, devY, numEpochs)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlossFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagScores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;31m# permute tagscores to calculate the loss over a sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimiser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[0;32m--> 488\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}