{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# INLP A2 2020111018"
      ],
      "metadata": {
        "id": "WSKSo1GpTz3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "0rKq6mkEUAgU",
        "outputId": "725970d0-c2fe-4121-f128-e3211c2bf9ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "from copy import copy"
      ],
      "metadata": {
        "id": "idYNya3YsY7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parse data\n",
        "\n",
        "# parse conllu, return sentences and tags\n",
        "def parseConllu(path):\n",
        "  X = []\n",
        "  y = []\n",
        "  pairs = []\n",
        "\n",
        "  with open(path) as f:\n",
        "    conllu = f.read().split('\\n\\n')\n",
        "    for block in conllu:\n",
        "      lines = block.split('\\n')\n",
        "      \n",
        "      sentence = []\n",
        "      tags = []\n",
        "      for line in lines[2:]:\n",
        "        items = line.split('\\t')\n",
        "        sentence.append(items[1])\n",
        "        tags.append(items[3])\n",
        "\n",
        "      pairs.append([sentence, tags])\n",
        "      \n",
        "  return np.array(pairs, dtype=object)\n",
        "\n",
        "# generate mappings between indices and words in sequences\n",
        "def assembleVocabulary(sequences, predefinedTags={\"_unk\": 0, \"_pad\": 1, \"_bos\": 2, \"_eos\": 3}):\n",
        "  vocab2idx = copy(predefinedTags)\n",
        "  count = len(predefinedTags)\n",
        "\n",
        "  # generate forward map\n",
        "  for sequence in sequences:\n",
        "    for word in sequence:\n",
        "      if word not in vocab2idx:\n",
        "        vocab2idx[word] = count\n",
        "        count += 1\n",
        "\n",
        "  # backwards map\n",
        "  idx2vocab = {vocab2idx[k]: k for k in vocab2idx}\n",
        "\n",
        "  return vocab2idx, idx2vocab\n",
        "\n",
        "# encode a sequence of words as a float tensor, takes a sentence and a dict as inputs, return a float tensor\n",
        "def encodeSequence(seq, toIdx):\n",
        "  encoded = []\n",
        "  for word in seq:\n",
        "    if word not in toIdx:\n",
        "      encoded.append(toIdx[\"_unk\"])\n",
        "    else:\n",
        "      encoded.append(toIdx[word])\n",
        "  encoded = torch.FloatTensor(encoded)\n",
        "  return encoded\n",
        "\n",
        "# pad all sequences with a \"_pad\" character\n",
        "def padSequence(sequences):\n",
        "  maxLength = 0\n",
        "  paddedSeqs = []\n",
        "\n",
        "  for seq in sequences:\n",
        "    if len(seq) > maxLength:\n",
        "      maxLength = len(seq)\n",
        "\n",
        "  for seq in sequences:\n",
        "    paddingNeeded = maxLength - len(seq)\n",
        "    paddedSeqs.append(seq + [\"_pad\"] * paddingNeeded)\n",
        "\n",
        "  paddedSeqs = np.array(paddedSeqs, dtype=object)\n",
        "  return paddedSeqs\n",
        "\n",
        "# begin and end sentences with \"_bos\" and \"_eos\" characters\n",
        "def delimitSequence(sequences):\n",
        "  delimited = []\n",
        "  for seq in sequences:\n",
        "    delimited.append([\"_bos\"] + seq + [\"_eos\"])\n",
        "\n",
        "  delimited = np.array(delimited, dtype=object)\n",
        "  return delimited"
      ],
      "metadata": {
        "id": "ZapBYceFugFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# parse data, generate training, dev and test splits\n",
        "\n",
        "# load shuffle and preprocess data\n",
        "pairs = parseConllu(\"/content/drive/MyDrive/ud-english-treebanks/UD_English-Atis/en_atis-ud-train.conllu\")\n",
        "\n",
        "np.random.shuffle(pairs)\n",
        "\n",
        "X = (pairs[:, 0])\n",
        "y = (pairs[:, 1])\n",
        "\n",
        "X = delimitSequence(X)\n",
        "y = delimitSequence(y)\n",
        "\n",
        "X = padSequence(X)\n",
        "y = padSequence(y)\n",
        "\n",
        "# make vocabs\n",
        "word2idx, idx2word = assembleVocabulary(X)\n",
        "tag2idx, idx2tag = assembleVocabulary(y)\n",
        "\n",
        "# split data\n",
        "trainAmt = 0.7\n",
        "devAmt = 0.1\n",
        "\n",
        "trainIdx = int(trainAmt * pairs.shape[0])\n",
        "devIdx = int(devAmt * pairs.shape[0])\n",
        "\n",
        "trainX = X[:trainIdx]\n",
        "trainY = y[:trainIdx]\n",
        "\n",
        "devX = X[trainIdx:(trainIdx + devIdx)]\n",
        "devY = y[trainIdx:(trainIdx + devIdx)]\n",
        "\n",
        "testX = X[(trainIdx + devIdx):]\n",
        "testY = y[(trainIdx + devIdx):]"
      ],
      "metadata": {
        "id": "8I0rCLM49UyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create dataloaders\n",
        "\n",
        "# encode sequences internally \n",
        "class POSTagDataset(Dataset):\n",
        "  def __init__(self, sentences, tags, word2idx, tag2idx):\n",
        "    # encode and store sentences\n",
        "    encSentences = torch.Tensor(encodeSequence(sentences[0], word2idx))\n",
        "    for sentence in sentences[1:]:\n",
        "      encSentences = torch.vstack([encSentences, encodeSequence(sentence, word2idx)])\n",
        "    \n",
        "    self.encSentences = encSentences\n",
        "\n",
        "    # encode and store POS tags\n",
        "    encPOS = torch.Tensor(encodeSequence(tags[0], tag2idx))\n",
        "    for tag in tags[1:]:\n",
        "      encPOS = torch.vstack([encPOS, encodeSequence(tag, tag2idx)])\n",
        "    \n",
        "    self.encPOS = encPOS\n",
        "    assert(self.encPOS.shape[0] == self.encSentences.shape[0])\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.encSentences.shape[0]\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.encSentences[idx], self.encPOS[idx]"
      ],
      "metadata": {
        "id": "Ik0ZzdF0HGEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainDataset = POSTagDataset(trainX, trainY, word2idx, tag2idx)\n",
        "devDataset = POSTagDataset(devX, devY, word2idx, tag2idx)\n",
        "testDataset = POSTagDataset(testX, testY, word2idx, tag2idx)"
      ],
      "metadata": {
        "id": "LNMW8TYLJUiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define main LSTM and experiment classes\n",
        "\n",
        "# main lstm, take in encoded sente\n",
        "class LSTM(nn.Module):\n",
        "  def __init__(self, embeddingDim, hiddenDim, wordVocab, tagVocab):\n",
        "    super(LSTM, self).__init__()\n",
        "\n",
        "\n",
        "    self.hiddenDim = hiddenDim\n",
        "    self.padIdx = 1\n",
        "    self.wordEmbeddings = nn.Embedding(len(wordVocab), embeddingDim, \n",
        "                                       padding_idx=self.padIdx)         # encode each word as an embedding of size embeddingDim\n",
        "\n",
        "    self.lstm = nn.LSTM(embeddingDim, hiddenDim, batch_first=True)      # main lstm \n",
        "    self.fc = nn.Linear(hiddenDim, len(tagVocab))                       # output a tag based on the hidden dim (using the internal state)\n",
        "\n",
        "    self.logSoftMax = F.log_softmax\n",
        "\n",
        "  def forward(self, sentences):\n",
        "    embeddings = self.wordEmbeddings(sentences)\n",
        "    out = self.lstm(embeddings)\n",
        "    out = self.fc(out)\n",
        "\n",
        "    tagScores = self.logSoftMax(out, dim=1)\n",
        "    return tagScores\n",
        "    \n",
        "# experiment class to manage training and testing\n",
        "class Experiment():\n",
        "  def __init__(self, embeddingDim, hiddenDim, wordVocab, tagVocab, batchSize=16, lossFunction=nn.CrossEntropyLoss, optimiser=torch.optim.SGD, lr=0.01):\n",
        "    self.model = LSTM(embedddingDim, hiddenDim, wordVocab, tagVocab)\n",
        "    self.lossFunction = lossFunction()\n",
        "    self.optimiser = optimiser(model.parameters(), lr=lr)\n",
        "\n",
        "    self.batchSize = batchSize\n",
        "\n",
        "  def train(trainX, trainY, valX, valY, numEpochs=50):\n",
        "    # init dataloaders\n",
        "    pass\n",
        "\n",
        "trainDataloader = DataLoader(trainDataset, batch_size=16, shuffle=True)\n",
        "trainSentences, trainLabels = next(iter(trainDataloader))\n",
        "\n",
        "#trainSentences.view(trainSentences.shape[0], trainSentences.shape[1], 1),\n",
        "\n",
        "myLSTM = LSTM(30, 50, word2idx, tag2idx)\n",
        "embs = myLSTM.wordEmbeddings(trainSentences.long())\n",
        "res, _ = myLSTM.lstm(embs)\n",
        "tagg = myLSTM.fc(res)\n",
        "scores = myLSTM.logSoftMax(tagg)\n",
        "\n",
        "print(scores, scores.shape)\n",
        "print()\n",
        "\n",
        "print(trainLabels.shape)\n",
        "print()\n",
        "\n",
        "\n",
        "print(tag2idx)\n",
        "\n",
        "e1 = Experiment(30, 50, word2idx, tag2idx)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHpURtuhOe3K",
        "outputId": "43f61ed8-1394-47d4-b71a-16ce4f9798b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-2.7726, -2.7726, -2.7726,  ..., -2.7726, -2.7726, -2.7726],\n",
            "         [-2.8033, -2.7417, -2.7417,  ..., -2.7384, -2.8320, -2.7908],\n",
            "         [-2.8758, -2.6763, -2.8177,  ..., -2.8249, -2.7999, -2.7140],\n",
            "         ...,\n",
            "         [-2.7726, -2.7726, -2.7726,  ..., -2.7726, -2.7726, -2.7726],\n",
            "         [-2.7726, -2.7726, -2.7726,  ..., -2.7726, -2.7726, -2.7726],\n",
            "         [-2.7726, -2.7726, -2.7726,  ..., -2.7726, -2.7726, -2.7726]],\n",
            "\n",
            "        [[-2.7726, -2.7726, -2.7726,  ..., -2.7726, -2.7726, -2.7726],\n",
            "         [-2.8110, -2.7350, -2.8190,  ..., -2.7833, -2.7325, -2.8254],\n",
            "         [-2.8728, -2.6551, -2.7697,  ..., -2.8390, -2.8786, -2.8229],\n",
            "         ...,\n",
            "         [-2.7726, -2.7726, -2.7726,  ..., -2.7726, -2.7726, -2.7726],\n",
            "         [-2.7726, -2.7726, -2.7726,  ..., -2.7726, -2.7726, -2.7726],\n",
            "         [-2.7726, -2.7726, -2.7726,  ..., -2.7726, -2.7726, -2.7726]],\n",
            "\n",
            "        [[-2.7726, -2.7726, -2.7726,  ..., -2.7726, -2.7726, -2.7726],\n",
            "         [-2.7890, -2.7987, -2.6946,  ..., -2.7910, -2.7694, -2.8170],\n",
            "         [-2.8519, -2.7737, -2.7820,  ..., -2.7841, -2.7996, -2.7538],\n",
            "         ...,\n",
            "         [-2.7726, -2.7726, -2.7726,  ..., -2.7726, -2.7726, -2.7726],\n",
            "         [-2.7726, -2.7726, -2.7726,  ..., -2.7726, -2.7726, -2.7726],\n",
            "         [-2.7726, -2.7726, -2.7726,  ..., -2.7726, -2.7726, -2.7726]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-2.7726, -2.7726, -2.7726,  ..., -2.7726, -2.7726, -2.7726],\n",
            "         [-2.7711, -2.7748, -2.7309,  ..., -2.8153, -2.7967, -2.6930],\n",
            "         [-2.6855, -2.7795, -2.7633,  ..., -2.7907, -2.8144, -2.6929],\n",
            "         ...,\n",
            "         [-2.7726, -2.7726, -2.7726,  ..., -2.7726, -2.7726, -2.7726],\n",
            "         [-2.7726, -2.7726, -2.7726,  ..., -2.7726, -2.7726, -2.7726],\n",
            "         [-2.7726, -2.7726, -2.7726,  ..., -2.7726, -2.7726, -2.7726]],\n",
            "\n",
            "        [[-2.7726, -2.7726, -2.7726,  ..., -2.7726, -2.7726, -2.7726],\n",
            "         [-2.7647, -2.7739, -2.7959,  ..., -2.7401, -2.7351, -2.8441],\n",
            "         [-2.7333, -2.8283, -2.7733,  ..., -2.7292, -2.7223, -2.8344],\n",
            "         ...,\n",
            "         [-2.7726, -2.7726, -2.7726,  ..., -2.7726, -2.7726, -2.7726],\n",
            "         [-2.7726, -2.7726, -2.7726,  ..., -2.7726, -2.7726, -2.7726],\n",
            "         [-2.7726, -2.7726, -2.7726,  ..., -2.7726, -2.7726, -2.7726]],\n",
            "\n",
            "        [[-2.7726, -2.7726, -2.7726,  ..., -2.7726, -2.7726, -2.7726],\n",
            "         [-2.7647, -2.7739, -2.7959,  ..., -2.7401, -2.7351, -2.8441],\n",
            "         [-2.8029, -2.7715, -2.7313,  ..., -2.7526, -2.7570, -2.8689],\n",
            "         ...,\n",
            "         [-2.7726, -2.7726, -2.7726,  ..., -2.7726, -2.7726, -2.7726],\n",
            "         [-2.7726, -2.7726, -2.7726,  ..., -2.7726, -2.7726, -2.7726],\n",
            "         [-2.7726, -2.7726, -2.7726,  ..., -2.7726, -2.7726, -2.7726]]],\n",
            "       grad_fn=<LogSoftmaxBackward0>) torch.Size([16, 48, 17])\n",
            "\n",
            "torch.Size([16, 48])\n",
            "\n",
            "{'_unk': 0, '_pad': 1, '_bos': 2, '_eos': 3, 'AUX': 4, 'PROPN': 5, 'VERB': 6, 'NOUN': 7, 'PRON': 8, 'DET': 9, 'ADP': 10, 'ADV': 11, 'CCONJ': 12, 'INTJ': 13, 'ADJ': 14, 'PART': 15, 'NUM': 16}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-50-45f2f2272774>:37: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  scores = myLSTM.logSoftMax(tagg)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}