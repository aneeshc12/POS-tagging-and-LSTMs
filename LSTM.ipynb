{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aneeshc12/POS-tagging-and-LSTMs/blob/master/LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# INLP A2 2020111018"
      ],
      "metadata": {
        "id": "WSKSo1GpTz3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "0rKq6mkEUAgU",
        "outputId": "64f6fdf9-bdb0-4385-fbb4-0b16cfdd0c2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "from copy import copy\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "idYNya3YsY7m"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parse data\n",
        "\n",
        "# parse conllu, return sentences and tags\n",
        "def parseConllu(path):\n",
        "  X = []\n",
        "  y = []\n",
        "  pairs = []\n",
        "\n",
        "  with open(path) as f:\n",
        "    conllu = f.read().split('\\n\\n')\n",
        "    for block in conllu:\n",
        "      lines = block.split('\\n')\n",
        "      \n",
        "      sentence = []\n",
        "      tags = []\n",
        "      for line in lines[2:]:\n",
        "        print(line)\n",
        "        items = line.split('\\t')\n",
        "        sentence.append(items[1])\n",
        "        tags.append(items[3])\n",
        "\n",
        "      pairs.append([sentence, tags])\n",
        "      \n",
        "  return np.array(pairs, dtype=object)\n",
        "\n",
        "# generate mappings between indices and words in sequences\n",
        "def assembleVocabulary(sequences, predefinedTags={\"_unk\": 0, \"_pad\": 1}): # no _bos or _eos\n",
        "  vocab2idx = copy(predefinedTags)\n",
        "  count = len(predefinedTags)\n",
        "\n",
        "  # generate forward map\n",
        "  for sequence in sequences:\n",
        "    for word in sequence:\n",
        "      if word not in vocab2idx:\n",
        "        vocab2idx[word] = count\n",
        "        count += 1\n",
        "\n",
        "  # backwards map\n",
        "  idx2vocab = {vocab2idx[k]: k for k in vocab2idx}\n",
        "\n",
        "  return vocab2idx, idx2vocab\n",
        "\n",
        "# encode a sequence of words as a float tensor, takes a sentence and a dict as inputs, return a float tensor\n",
        "def encodeSequence(seq, toIdx):\n",
        "  encoded = []\n",
        "  for word in seq:\n",
        "    if word not in toIdx:\n",
        "      encoded.append(toIdx[\"_unk\"])\n",
        "    else:\n",
        "      encoded.append(toIdx[word])\n",
        "  encoded = torch.FloatTensor(encoded)\n",
        "  return encoded\n",
        "\n",
        "# pad all sequences with a \"_pad\" character uptil padding length, truncate larger strings\n",
        "def padSequence(sequences, paddingLength):\n",
        "  maxLength = 0\n",
        "  paddedSeqs = []\n",
        "\n",
        "  for seq in sequences:\n",
        "    if len(seq) > maxLength:\n",
        "      maxLength = len(seq)\n",
        "\n",
        "  for seq in sequences:\n",
        "    if len(seq) > paddingLength:\n",
        "      paddedSeqs.append(seq[:paddingLength])\n",
        "    else:\n",
        "      paddingNeeded = paddingLength - len(seq)\n",
        "      paddedSeqs.append(seq + [\"_pad\"] * paddingNeeded)\n",
        "\n",
        "  paddedSeqs = np.array(paddedSeqs, dtype=object)\n",
        "  return paddedSeqs\n",
        "\n",
        "# begin and end sentences with \"_bos\" and \"_eos\" characters\n",
        "def delimitSequence(sequences):\n",
        "  delimited = []\n",
        "  for seq in sequences:\n",
        "    # delimited.append([\"_bos\"] + seq + [\"_eos\"])\n",
        "    delimited.append(seq)\n",
        "\n",
        "  delimited = np.array(delimited, dtype=object)\n",
        "  return delimited"
      ],
      "metadata": {
        "id": "ZapBYceFugFW"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load raw data\n",
        "pairs = parseConllu(\"/content/drive/MyDrive/ud-english-treebanks/UD_English-GUM/en_gum-ud-train.conllu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "id": "lqxrJb_LTwV-",
        "outputId": "32ac44d3-02da-40e4-e8f0-440453482e10"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# meta::author = Claire Bailey-Ross, Andrew Beresford, Daniel Smith, Claire Warwick\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-120-40e42d06141b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load raw data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparseConllu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/ud-english-treebanks/UD_English-GUM/en_gum-ud-train.conllu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-118-8834ae52c762>\u001b[0m in \u001b[0;36mparseConllu\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mtags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# visualise data lengths to get the best padding length\n",
        "%matplotlib inline\n",
        "\n",
        "nX = pairs[:, 0]\n",
        "\n",
        "maxLength = 0\n",
        "for seq in nX:\n",
        "  if len(seq) > maxLength:\n",
        "    maxLength = len(seq)\n",
        "\n",
        "counts = np.zeros(maxLength+1)\n",
        "for seq in nX:\n",
        "  counts[len(seq)] += 1\n",
        "\n",
        "plt.plot(np.arange(0,maxLength+1,1), counts)\n",
        "\n",
        "# mean is about 12 for the altis dataset, setting padding to 18"
      ],
      "metadata": {
        "id": "9X2lSNVnTsEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# parse data, generate training, dev and test splits\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Device is: \", device)\n",
        "\n",
        "# load shuffle and preprocess data\n",
        "\n",
        "np.random.shuffle(pairs)\n",
        "\n",
        "X = (pairs[:, 0])\n",
        "y = (pairs[:, 1])\n",
        "\n",
        "X = delimitSequence(X)\n",
        "y = delimitSequence(y)\n",
        "\n",
        "X = padSequence(X, 15)    # mean for altis is ~12, using a padding of 20\n",
        "y = padSequence(y, 15)    \n",
        "\n",
        "# make vocabs\n",
        "word2idx, idx2word = assembleVocabulary(X)\n",
        "tag2idx, idx2tag = assembleVocabulary(y)\n",
        "\n",
        "# split data\n",
        "trainAmt = 0.8\n",
        "devAmt = 0.1\n",
        "\n",
        "trainIdx = int(trainAmt * pairs.shape[0])\n",
        "devIdx = int(devAmt * pairs.shape[0])\n",
        "\n",
        "trainX = X[:trainIdx]\n",
        "trainY = y[:trainIdx]\n",
        "\n",
        "devX = X[trainIdx:(trainIdx + devIdx)]\n",
        "devY = y[trainIdx:(trainIdx + devIdx)]\n",
        "\n",
        "testX = X[(trainIdx + devIdx):]\n",
        "testY = y[(trainIdx + devIdx):]"
      ],
      "metadata": {
        "id": "8I0rCLM49UyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create dataloaders\n",
        "\n",
        "# encode sequences internally \n",
        "class POSTagDataset(Dataset):\n",
        "  def __init__(self, sentences, tags, word2idx, tag2idx):\n",
        "    # encode and store sentences\n",
        "    encSentences = torch.Tensor(encodeSequence(sentences[0], word2idx))\n",
        "    for sentence in sentences[1:]:\n",
        "      encSentences = torch.vstack([encSentences, encodeSequence(sentence, word2idx)])\n",
        "    \n",
        "    self.encSentences = encSentences\n",
        "\n",
        "    # encode and store POS tags\n",
        "    encPOS = torch.Tensor(encodeSequence(tags[0], tag2idx))\n",
        "    for tag in tags[1:]:\n",
        "      encPOS = torch.vstack([encPOS, encodeSequence(tag, tag2idx)])\n",
        "    \n",
        "    self.encPOS = encPOS\n",
        "    assert(self.encPOS.shape[0] == self.encSentences.shape[0])\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.encSentences.shape[0]\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.encSentences[idx], self.encPOS[idx]"
      ],
      "metadata": {
        "id": "Ik0ZzdF0HGEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainDataset = POSTagDataset(trainX, trainY, word2idx, tag2idx)\n",
        "devDataset = POSTagDataset(devX, devY, word2idx, tag2idx)\n",
        "testDataset = POSTagDataset(testX, testY, word2idx, tag2idx)"
      ],
      "metadata": {
        "id": "LNMW8TYLJUiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define main LSTM and experiment classes\n",
        "\n",
        "# main lstm, take in encoded sente\n",
        "class LSTM(nn.Module):\n",
        "  def __init__(self, embeddingDim, hiddenDim, wordVocab, tagVocab, device):\n",
        "    super(LSTM, self).__init__()\n",
        "\n",
        "\n",
        "    self.hiddenDim = hiddenDim\n",
        "    self.padIdx = 1\n",
        "    self.wordEmbeddings = nn.Embedding(len(wordVocab), embeddingDim, \n",
        "                                       padding_idx=self.padIdx)         # encode each word as an embedding of size embeddingDim\n",
        "\n",
        "    with torch.no_grad():\n",
        "      self.wordEmbeddings.weight[self.padIdx,:] = 1.0  \n",
        "\n",
        "    self.lstm = nn.LSTM(embeddingDim, hiddenDim, batch_first=True)      # main lstm \n",
        "    self.fc = nn.Linear(hiddenDim, len(tagVocab))                       # output a tag based on the hidden dim (using the internal state)\n",
        "\n",
        "    self.logSoftMax = F.log_softmax\n",
        "\n",
        "    self.device = device\n",
        "\n",
        "  def forward(self, sentences):\n",
        "    s = sentences.to(self.device)         # move to cuda\n",
        "    embeddings = self.wordEmbeddings(s)\n",
        "    out, _ = self.lstm(embeddings)\n",
        "    out = self.fc(out)\n",
        "\n",
        "    tagScores = self.logSoftMax(out, dim=1)\n",
        "    return tagScores\n",
        "    \n",
        "# experiment class to manage training and testing\n",
        "class Experiment():\n",
        "  def __init__(self, embeddingDim, hiddenDim, wordVocab, tagVocab, device, batchSize=16, lossFunction=nn.CrossEntropyLoss, optimiser=torch.optim.SGD, lr=0.01):\n",
        "    self.model = LSTM(embeddingDim, hiddenDim, wordVocab, tagVocab, device).to(device)\n",
        "    self.lossFunction = lossFunction().to(device)\n",
        "    self.optimiser = optimiser(self.model.parameters(), lr=lr)\n",
        "\n",
        "    self.batchSize = batchSize\n",
        "    self.device = device\n",
        "\n",
        "\n",
        "  def evaluate(self, evalDataset):\n",
        "    \"\"\"\n",
        "    Evaluate the containers LSTM on a torch dataset\n",
        "\n",
        "    Return the average error with the containers loss function\n",
        "    \"\"\"\n",
        "    evalDataloader = DataLoader(evalDataset, batch_size=self.batchSize, shuffle=True)\n",
        "    with torch.no_grad():\n",
        "      evalLoss = 0.0\n",
        "      for i, (sentences, labels) in enumerate(iter(evalDataloader)):\n",
        "        s = sentences.to(self.device)\n",
        "        l = labels.to(self.device)\n",
        "\n",
        "        self.model.zero_grad()\n",
        "        tagScores = self.model(s.long())\n",
        "        \n",
        "        loss = self.lossFunction(tagScores.permute(0,2,1), l.long())        # permute tagscores to calculate the loss over a sentence\n",
        "\n",
        "        evalLoss += loss\n",
        "\n",
        "    return evalLoss/len(evalDataset)\n",
        "\n",
        "  def train(self, trainDataset, devDataset, numEpochs=50, printStep=10, saveStep=100, savePath=\"/content/drive/MyDrive/Colab Notebooks/INLP/INLP-A2 weights/\"):\n",
        "    \"\"\"\n",
        "    Train the containers LSTM given a set of datasets and parameters\n",
        "\n",
        "    Params:\n",
        "    trainDataset: training dataset (torch)\n",
        "    devDataset: dev dataset (torch)\n",
        "    numEpochs:\n",
        "    printStep: number of epochs before stats are printed\n",
        "    saveStep: number of epochs before weights are saved\n",
        "    savePath: default location weights are saved\n",
        "\n",
        "    Returns nothing\n",
        "    \"\"\"\n",
        "\n",
        "    # init dataloaders\n",
        "    trainDataloader = DataLoader(trainDataset, batch_size=self.batchSize, shuffle=True)\n",
        "\n",
        "    # iterate over all train batches, train the LSTM with sentences and labels\n",
        "    # keep iterating until performance on validation drops (early stoppage)\n",
        "    lastDevLoss = np.inf\n",
        "    for epoch in range(numEpochs):\n",
        "\n",
        "      # train over training data\n",
        "      trainingLoss = 0.0\n",
        "      for i, (sentences, labels) in enumerate(iter(trainDataloader)):\n",
        "        s = sentences.to(self.device)     # move to cuda\n",
        "        l = labels.to(self.device)\n",
        "\n",
        "        # zero grad, compute scores, evaluate loss, backprop, update weights\n",
        "        self.model.zero_grad()\n",
        "        tagScores = self.model(s.long())\n",
        "        \n",
        "        loss = self.lossFunction(tagScores.permute(0,2,1), l.long())        # permute tagscores to calculate the loss over a sentence\n",
        "        loss.backward()\n",
        "        self.optimiser.step()\n",
        "\n",
        "        trainingLoss += loss\n",
        "      trainingLoss /= len(trainDataloader)\n",
        "\n",
        "      # evaluate on dev data\n",
        "      devLoss = self.evaluate(devDataset)\n",
        "\n",
        "      # print every printStep\n",
        "      if epoch % printStep == 0:\n",
        "        print(\"Epoch %d | avg. training error: %f | avg. dev error: %f\" % (epoch, trainingLoss, devLoss))\n",
        "      \n",
        "      # save every save step\n",
        "      if epoch % saveStep == 0:\n",
        "        filePath = savePath + datetime.now().strftime(\"%m-%d-%H-%M-%S\") + \".pt\"\n",
        "        torch.save(self.model.state_dict, filePath)\n",
        "\n",
        "      # early stoppage\n",
        "      if devLoss > lastDevLoss:\n",
        "        filePath = savePath + datetime.now().strftime(\"%m-%d-%H-%M-%S\") + \".pt\"\n",
        "        torch.save(self.model.state_dict, filePath)\n",
        "\n",
        "        print(\"Increase in dev loss, stopping training\")\n",
        "        print(\"Epoch %d | avg. training error: %f | avg. dev error: %f\" % (epoch, trainingLoss, devLoss))\n",
        "        break\n",
        "      else:\n",
        "        lastDevLoss = devLoss\n",
        "\n",
        "\n",
        "def inference(model, sentence):\n",
        "  \"\"\"\n",
        "  Given a model and a sentence, print predicted POS tags\n",
        "\n",
        "  Return a tensor of tag indices\n",
        "  \"\"\"\n",
        "  with torch.no_grad():\n",
        "    processedSentence = sentence.lower().split(' ')\n",
        "    encodedSentence = encodeSequence(processedSentence, word2idx).long()\n",
        "    print(encodedSentence)\n",
        "    results = model(encodedSentence)\n",
        "\n",
        "  return results\n",
        "    \n",
        "    \n"
      ],
      "metadata": {
        "id": "yHpURtuhOe3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example training and evaluation\n",
        "\n",
        "e1 = Experiment(50, 100, word2idx, tag2idx, device, lr=0.01)\n",
        "e1.train(trainDataset, devDataset, numEpochs=250)\n",
        "testLoss = e1.evaluate(testDataset)\n",
        "print(\"Loss on the test dataset: \", testLoss)"
      ],
      "metadata": {
        "id": "oXH8o_663rfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = inference(e1.model, \"which flights leave april twelfth from indianapolis and arrive in montreal around 10 pm\").to('cpu')"
      ],
      "metadata": {
        "id": "Y_sllVx66KSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results\n",
        "for r in results:\n",
        "  # print(r)\n",
        "  idx = int(np.argmax(r))\n",
        "  print(idx, idx2tag[idx])"
      ],
      "metadata": {
        "id": "XbCcAhJL6jFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(testX[1])\n",
        "print(testY[1])"
      ],
      "metadata": {
        "id": "UIFblu35_Ga6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}