{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aneeshc12/POS-tagging-and-LSTMs/blob/master/Copy_of_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSKSo1GpTz3N"
      },
      "source": [
        "# INLP A2 2020111018"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rKq6mkEUAgU",
        "outputId": "f0c80136-0212-420f-b8f8-8ad44e7de750"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "idYNya3YsY7m"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "from copy import copy\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZapBYceFugFW"
      },
      "outputs": [],
      "source": [
        "# Parse data\n",
        "\n",
        "# parse conllu, return sentences and tags\n",
        "def parseConllu(path):\n",
        "  X = []\n",
        "  y = []\n",
        "  pairs = []\n",
        "\n",
        "  with open(path) as f:\n",
        "    conllu = f.read().split('\\n\\n')\n",
        "    for block in conllu:\n",
        "      lines = block.split('\\n')\n",
        "      \n",
        "      sentence = []\n",
        "      tags = []\n",
        "      for line in lines[2:]:\n",
        "        items = line.split('\\t')\n",
        "        sentence.append(items[1])\n",
        "        tags.append(items[3])\n",
        "\n",
        "      pairs.append([sentence, tags])\n",
        "      \n",
        "  return np.array(pairs, dtype=object)\n",
        "\n",
        "# generate mappings between indices and words in sequences\n",
        "def assembleVocabulary(sequences, predefinedTags={\"_unk\": 0, \"_pad\": 1, \"_bos\": 2, \"_eos\": 3}):\n",
        "  vocab2idx = copy(predefinedTags)\n",
        "  count = len(predefinedTags)\n",
        "\n",
        "  # generate forward map\n",
        "  for sequence in sequences:\n",
        "    for word in sequence:\n",
        "      if word not in vocab2idx:\n",
        "        vocab2idx[word] = count\n",
        "        count += 1\n",
        "\n",
        "  # backwards map\n",
        "  idx2vocab = {vocab2idx[k]: k for k in vocab2idx}\n",
        "\n",
        "  return vocab2idx, idx2vocab\n",
        "\n",
        "# encode a sequence of words as a float tensor, takes a sentence and a dict as inputs, return a float tensor\n",
        "def encodeSequence(seq, toIdx):\n",
        "  encoded = []\n",
        "  for word in seq:\n",
        "    if word not in toIdx:\n",
        "      encoded.append(toIdx[\"_unk\"])\n",
        "    else:\n",
        "      encoded.append(toIdx[word])\n",
        "  encoded = torch.FloatTensor(encoded)\n",
        "  return encoded\n",
        "\n",
        "# pad all sequences with a \"_pad\" character\n",
        "def padSequence(sequences):\n",
        "  maxLength = 0\n",
        "  paddedSeqs = []\n",
        "\n",
        "  for seq in sequences:\n",
        "    if len(seq) > maxLength:\n",
        "      maxLength = len(seq)\n",
        "\n",
        "  for seq in sequences:\n",
        "    paddingNeeded = maxLength - len(seq)\n",
        "    paddedSeqs.append(seq + [\"_pad\"] * paddingNeeded)\n",
        "\n",
        "  paddedSeqs = np.array(paddedSeqs, dtype=object)\n",
        "  return paddedSeqs\n",
        "\n",
        "# begin and end sentences with \"_bos\" and \"_eos\" characters\n",
        "def delimitSequence(sequences):\n",
        "  delimited = []\n",
        "  for seq in sequences:\n",
        "    delimited.append([\"_bos\"] + seq + [\"_eos\"])\n",
        "\n",
        "  delimited = np.array(delimited, dtype=object)\n",
        "  return delimited"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8I0rCLM49UyD",
        "outputId": "5a8341d2-a70e-4494-8f50-b626ccc1ed05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device is:  cuda\n"
          ]
        }
      ],
      "source": [
        "# parse data, generate training, dev and test splits\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Device is: \", device)\n",
        "\n",
        "# load shuffle and preprocess data\n",
        "pairs = parseConllu(\"/content/drive/MyDrive/ud-english-treebanks/UD_English-Atis/en_atis-ud-train.conllu\")\n",
        "\n",
        "np.random.shuffle(pairs)\n",
        "\n",
        "X = (pairs[:, 0])\n",
        "y = (pairs[:, 1])\n",
        "\n",
        "X = delimitSequence(X)\n",
        "y = delimitSequence(y)\n",
        "\n",
        "X = padSequence(X)\n",
        "y = padSequence(y)\n",
        "\n",
        "# make vocabs\n",
        "word2idx, idx2word = assembleVocabulary(X)\n",
        "tag2idx, idx2tag = assembleVocabulary(y)\n",
        "\n",
        "# split data\n",
        "trainAmt = 0.7\n",
        "devAmt = 0.1\n",
        "\n",
        "trainIdx = int(trainAmt * pairs.shape[0])\n",
        "devIdx = int(devAmt * pairs.shape[0])\n",
        "\n",
        "trainX = X[:trainIdx]\n",
        "trainY = y[:trainIdx]\n",
        "\n",
        "devX = X[trainIdx:(trainIdx + devIdx)]\n",
        "devY = y[trainIdx:(trainIdx + devIdx)]\n",
        "\n",
        "testX = X[(trainIdx + devIdx):]\n",
        "testY = y[(trainIdx + devIdx):]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Ik0ZzdF0HGEz"
      },
      "outputs": [],
      "source": [
        "# create dataloaders\n",
        "\n",
        "# encode sequences internally \n",
        "class POSTagDataset(Dataset):\n",
        "  def __init__(self, sentences, tags, word2idx, tag2idx):\n",
        "    # encode and store sentences\n",
        "    encSentences = torch.Tensor(encodeSequence(sentences[0], word2idx))\n",
        "    for sentence in sentences[1:]:\n",
        "      encSentences = torch.vstack([encSentences, encodeSequence(sentence, word2idx)])\n",
        "    \n",
        "    self.encSentences = encSentences\n",
        "\n",
        "    # encode and store POS tags\n",
        "    encPOS = torch.Tensor(encodeSequence(tags[0], tag2idx))\n",
        "    for tag in tags[1:]:\n",
        "      encPOS = torch.vstack([encPOS, encodeSequence(tag, tag2idx)])\n",
        "    \n",
        "    self.encPOS = encPOS\n",
        "    assert(self.encPOS.shape[0] == self.encSentences.shape[0])\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.encSentences.shape[0]\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.encSentences[idx], self.encPOS[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "LNMW8TYLJUiN"
      },
      "outputs": [],
      "source": [
        "trainDataset = POSTagDataset(trainX, trainY, word2idx, tag2idx)\n",
        "devDataset = POSTagDataset(devX, devY, word2idx, tag2idx)\n",
        "testDataset = POSTagDataset(testX, testY, word2idx, tag2idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHpURtuhOe3K",
        "outputId": "67529754-1679-4332-ec13-a366631bd29e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 | avg. training error: 2.823730 | avg. dev error: 0.177983\n",
            "Epoch 10 | avg. training error: 2.473327 | avg. dev error: 0.154401\n",
            "Epoch 20 | avg. training error: 0.706971 | avg. dev error: 0.044986\n",
            "Epoch 30 | avg. training error: 0.579968 | avg. dev error: 0.037244\n",
            "Epoch 40 | avg. training error: 0.476098 | avg. dev error: 0.030643\n",
            "Epoch 50 | avg. training error: 0.394650 | avg. dev error: 0.025507\n",
            "Epoch 60 | avg. training error: 0.349984 | avg. dev error: 0.022702\n",
            "Epoch 70 | avg. training error: 0.319026 | avg. dev error: 0.020700\n",
            "Epoch 80 | avg. training error: 0.295236 | avg. dev error: 0.019223\n",
            "Epoch 90 | avg. training error: 0.275942 | avg. dev error: 0.017907\n",
            "Epoch 100 | avg. training error: 0.259718 | avg. dev error: 0.016901\n",
            "Epoch 110 | avg. training error: 0.245715 | avg. dev error: 0.016004\n",
            "Epoch 120 | avg. training error: 0.233407 | avg. dev error: 0.015198\n",
            "Epoch 130 | avg. training error: 0.222459 | avg. dev error: 0.014565\n",
            "Epoch 140 | avg. training error: 0.212645 | avg. dev error: 0.013847\n",
            "Increase in dev loss, stopping training\n",
            "Epoch 145 | avg. training error: 0.208109 | avg. dev error: 0.013623\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor(0.0131, device='cuda:0')"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# define main LSTM and experiment classes\n",
        "\n",
        "# main lstm, take in encoded sente\n",
        "class LSTM(nn.Module):\n",
        "  def __init__(self, embeddingDim, hiddenDim, wordVocab, tagVocab, device):\n",
        "    super(LSTM, self).__init__()\n",
        "\n",
        "\n",
        "    self.hiddenDim = hiddenDim\n",
        "    self.padIdx = 1\n",
        "    self.wordEmbeddings = nn.Embedding(len(wordVocab), embeddingDim, \n",
        "                                       padding_idx=self.padIdx)         # encode each word as an embedding of size embeddingDim\n",
        "\n",
        "    self.lstm = nn.LSTM(embeddingDim, hiddenDim, batch_first=True)      # main lstm \n",
        "    self.fc = nn.Linear(hiddenDim, len(tagVocab))                       # output a tag based on the hidden dim (using the internal state)\n",
        "\n",
        "    self.logSoftMax = F.log_softmax\n",
        "\n",
        "    self.device = device\n",
        "\n",
        "  def forward(self, sentences):\n",
        "    s = sentences.to(self.device)         # move to cuda\n",
        "    embeddings = self.wordEmbeddings(s)\n",
        "    out, _ = self.lstm(embeddings)\n",
        "    out = self.fc(out)\n",
        "\n",
        "    tagScores = self.logSoftMax(out, dim=1)\n",
        "    return tagScores\n",
        "    \n",
        "# experiment class to manage training and testing\n",
        "class Experiment():\n",
        "  def __init__(self, embeddingDim, hiddenDim, wordVocab, tagVocab, device, batchSize=16, lossFunction=nn.CrossEntropyLoss, optimiser=torch.optim.SGD, lr=0.01):\n",
        "    self.model = LSTM(embeddingDim, hiddenDim, wordVocab, tagVocab, device).to(device)\n",
        "    self.lossFunction = lossFunction().to(device)\n",
        "    self.optimiser = optimiser(self.model.parameters(), lr=lr)\n",
        "\n",
        "    self.batchSize = batchSize\n",
        "    self.device = device\n",
        "\n",
        "\n",
        "  def evaluate(self, evalDataset):\n",
        "    evalDataloader = DataLoader(evalDataset, batch_size=self.batchSize, shuffle=True)\n",
        "    with torch.no_grad():\n",
        "      evalLoss = 0.0\n",
        "      for i, (sentences, labels) in enumerate(iter(evalDataloader)):\n",
        "        s = sentences.to(self.device)\n",
        "        l = labels.to(self.device)\n",
        "\n",
        "        self.model.zero_grad()\n",
        "        tagScores = self.model(s.long())\n",
        "        \n",
        "        loss = self.lossFunction(tagScores.permute(0,2,1), l.long())        # permute tagscores to calculate the loss over a sentence\n",
        "\n",
        "        evalLoss += loss\n",
        "\n",
        "    return evalLoss/len(evalDataset)\n",
        "\n",
        "  def train(self, trainDataset, devDataset, numEpochs=50, printStep=10, saveStep=100, savePath=\"/content/drive/MyDrive/Colab Notebooks/INLP/INLP-A2 weights/\"):\n",
        "    # init dataloaders\n",
        "    trainDataloader = DataLoader(trainDataset, batch_size=self.batchSize, shuffle=True)\n",
        "\n",
        "    # iterate over all train batches, train the LSTM with sentences and labels\n",
        "    # keep iterating until performance on validation drops (early stoppage)\n",
        "    lastDevLoss = np.inf\n",
        "    for epoch in range(numEpochs):\n",
        "\n",
        "      # train over training data\n",
        "      trainingLoss = 0.0\n",
        "      for i, (sentences, labels) in enumerate(iter(trainDataloader)):\n",
        "        s = sentences.to(self.device)     # move to cuda\n",
        "        l = labels.to(self.device)\n",
        "\n",
        "        # zero grad, compute scores, evaluate loss, backprop, update weights\n",
        "        self.model.zero_grad()\n",
        "        tagScores = self.model(s.long())\n",
        "        \n",
        "        loss = self.lossFunction(tagScores.permute(0,2,1), l.long())        # permute tagscores to calculate the loss over a sentence\n",
        "        loss.backward()\n",
        "        self.optimiser.step()\n",
        "\n",
        "        trainingLoss += loss\n",
        "      trainingLoss /= len(trainDataloader)\n",
        "\n",
        "      # evaluate on dev data\n",
        "      devLoss = self.evaluate(devDataset)\n",
        "\n",
        "      # print every printStep\n",
        "      if epoch % printStep == 0:\n",
        "        print(\"Epoch %d | avg. training error: %f | avg. dev error: %f\" % (epoch, trainingLoss, devLoss))\n",
        "      \n",
        "      # save every save step\n",
        "      if epoch % saveStep == 0:\n",
        "        filePath = savePath + datetime.now().strftime(\"%m-%d-%H-%M-%S\") + \".pt\"\n",
        "        torch.save(self.model.state_dict, filePath)\n",
        "\n",
        "      # early stoppage\n",
        "      if devLoss > lastDevLoss:\n",
        "        filePath = savePath + datetime.now().strftime(\"%m-%d-%H-%M-%S\") + \".pt\"\n",
        "        torch.save(self.model.state_dict, filePath)\n",
        "\n",
        "        print(\"Increase in dev loss, stopping training\")\n",
        "        print(\"Epoch %d | avg. training error: %f | avg. dev error: %f\" % (epoch, trainingLoss, devLoss))\n",
        "        break\n",
        "      else:\n",
        "        lastDevLoss = devLoss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# for i, (ts, tl) in enumerate(iter(trainDataloader)):\n",
        "#   print(\"iter \", i)\n",
        "#   print(ts.shape, ts[0])\n",
        "#   print(tl.shape, tl[0])\n",
        "\n",
        "trainDataloader = DataLoader(trainDataset, batch_size=16, shuffle=True)\n",
        "trainSentences, trainLabels = next(iter(trainDataloader))\n",
        "\n",
        "# trainSentences.view(trainSentences.shape[0], trainSentences.shape[1], 1),\n",
        "\n",
        "# myLSTM = LSTM(30, 50, word2idx, tag2idx)\n",
        "# embs = myLSTM.wordEmbeddings(trainSentences.long())\n",
        "# res, _ = myLSTM.lstm(embs)\n",
        "# tagg = myLSTM.fc(res)\n",
        "# scores = myLSTM.logSoftMax(tagg)\n",
        "\n",
        "# scores = myLSTM(trainSentences.long())\n",
        "# loss = nn.CrossEntropyLoss()\n",
        "\n",
        "e1 = Experiment(50, 100, word2idx, tag2idx, device, lr=0.01)\n",
        "e1.train(trainDataset, devDataset, numEpochs=250)\n",
        "e1.evaluate(testDataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save word and tag vocabs\n",
        "import pickle\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/INLP/INLP-A2 weights/word2idx.pkl\", \"wb\") as f:\n",
        "  pickle.dump(word2idx, f)\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/INLP/INLP-A2 weights/tag2idx.pkl\", \"wb\") as f:\n",
        "  pickle.dump(tag2idx, f)\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/INLP/INLP-A2 weights/idx2tag.pkl\", \"wb\") as f:\n",
        "  pickle.dump(idx2tag, f)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
